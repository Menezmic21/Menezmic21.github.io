---
title: "OptimaLab: Transformer with Independent Subnetwork Training"
date: 2024-01-20T12:00:00+00:00
description: "Applied IST techniques to efficiently train transformer architectures in a distributed scenario."
author: "Chen Dun, Tasos Kyrillidis, Michael Menezes, Hamza Shili, Barbara Su"
tags: ["Code", "Machine Learning", "Research"]
theme: "light"
featured: true
cover: "../../../assets/images/transformers.png"
---
![Transformer architecture](/assets/images/transformers.png)

<!-- Link to [REPO](https://github.com/Menezmic21/leetcode-submissions). -->

<!-- Descriptive paragraph of project -->
This research project seeks to evaluate the effectiveness of Independent Subnetwork Training (IST) methods on the Transformer architecture. A transformer is a type of neural network that is often used for natural language processing. Transformers are made up of attention heads which are modules that "learn" how which "words" in a sentence are "important." 

To explain exactly what this research project does, I will first explain machine learning in general. Machine learning tries to develop a mathematical model that can be used to predict a function's output given an input by observing the function's behaviour on a subset of inputs. The simplist model is a line. 

$y = mx + b$

# What I did

# What I used

# Challenges