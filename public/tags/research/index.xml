<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Research on Michael Menezes</title>
    <link>//localhost:1313/tags/research/</link>
    <description>Recent content in Research on Michael Menezes</description>
    <generator>Hugo</generator>
    <language>en</language>
    <copyright>Michael Menezes</copyright>
    <lastBuildDate>Tue, 13 Feb 2024 12:00:00 +0000</lastBuildDate>
    <atom:link href="//localhost:1313/tags/research/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>OptimaLab: Transformer with Independent Subnetwork Training</title>
      <link>//localhost:1313/posts/optima_lab_twist/</link>
      <pubDate>Tue, 13 Feb 2024 12:00:00 +0000</pubDate>
      <guid>//localhost:1313/posts/optima_lab_twist/</guid>
      <description>Transformer architecture&#xD;This research project seeks to evaluate the effectiveness of Independent Subnetwork Training (IST) methods on the Transformer architecture. A transformer is a type of neural network that is often used for natural language processing. Transformers are made up of attention heads which are modules that &amp;ldquo;learn&amp;rdquo; how which &amp;ldquo;words&amp;rdquo; in a sentence are &amp;ldquo;important.&amp;rdquo;&#xA;To explain exactly what this research project does, I will first explain machine learning in general.</description>
    </item>
    <item>
      <title>OptimaLab: Machine Unlearning</title>
      <link>//localhost:1313/posts/optima_lab_unlearning/</link>
      <pubDate>Sat, 20 Jan 2024 12:00:00 +0000</pubDate>
      <guid>//localhost:1313/posts/optima_lab_unlearning/</guid>
      <description>Machine unlearning kaggle competition&#xD;Link to KAGGLE.&#xA;Deep learning has recently driven tremendous progress in a wide array of applications, ranging from realistic image generation and impressive retrieval systems to language models that can hold human-like conversations. While this progress is very exciting, the widespread use of deep neural network models requires caution: researchers should seek to develop AI technologies responsibly by understanding and mitigating potential risks, such as the propagation and amplification of unfair biases and protecting user privacy.</description>
    </item>
    <item>
      <title>SIR Mobility Metapopulation Models: Designing an Epidemic Model</title>
      <link>//localhost:1313/posts/science_fair_11th/</link>
      <pubDate>Sat, 30 Jan 2021 12:00:00 +0000</pubDate>
      <guid>//localhost:1313/posts/science_fair_11th/</guid>
      <description>SIR model with two communities&#xD;Over the past century, there has been a continual increase in the probability of pandemics as a result of heightened global travel and urbanization. These pandemics have sever repercussions which decrease the labor force, amplify social tensions, and augment partisanship. In order to concentrate mitigation efforts, accurate epidemic forecasts are essential. In particular to this study, metapopulation models are of interest as they incorporate how individuals travel between communities providing a deeper understanding of how epidemics spread.</description>
    </item>
    <item>
      <title>A Slick Model: Modeling the Dispersion of Oil Slicks in the Ocean</title>
      <link>//localhost:1313/posts/science_fair_9th/</link>
      <pubDate>Wed, 30 Jan 2019 12:00:00 +0000</pubDate>
      <guid>//localhost:1313/posts/science_fair_9th/</guid>
      <description>SIR model with two communities&#xD;Link to PAPER.&#xA;Oil spills cause millions of dollars in damages and destroy habitats. These repercussions highlight the need to contain oil spills quickly. Oil dispersion models are a set of procedures that model the dispersion of spills. Though oil dispersion models currently exist, these models require a lot of computational power. This project proposed a simple mathematical model to represent the dispersion of oil slicks.</description>
    </item>
  </channel>
</rss>
